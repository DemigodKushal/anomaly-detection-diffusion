{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f26cf5",
   "metadata": {},
   "source": [
    "1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14077d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y diffusers transformers huggingface_hub accelerate peft torch torchvision torchaudio xformers bitsandbytes -q\n",
    "\n",
    "# PyTorch (CUDA 12.1) for Kaggle GPUs\n",
    "!pip install -q torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Modern ecosystem\n",
    "!pip install -q diffusers==0.26.3 transformers==4.39.3 accelerate==0.28.0 peft==0.10.0 huggingface_hub==0.20.3 bitsandbytes==0.43.0\n",
    "\n",
    "# Optional xFormers\n",
    "!pip install -q xformers==0.0.22.post7 -f https://download.pytorch.org/whl/cu121/torch_stable.html\n",
    "\n",
    "import torch, diffusers, transformers, accelerate, peft, huggingface_hub\n",
    "print('torch', torch.__version__)\n",
    "print('diffusers', diffusers.__version__)\n",
    "print('transformers', transformers.__version__)\n",
    "print('accelerate', accelerate.__version__)\n",
    "print('peft', peft.__version__)\n",
    "print('huggingface_hub', huggingface_hub.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e0163",
   "metadata": {},
   "source": [
    "2. Download DreamBooth LoRA Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L https://raw.githubusercontent.com/huggingface/diffusers/v0.26.3/examples/dreambooth/train_dreambooth_lora.py -o /kaggle/working/train_dreambooth_lora.py\n",
    "!sed -n '1,40p' /kaggle/working/train_dreambooth_lora.py | sed -n '1,20p'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1ffa3",
   "metadata": {},
   "source": [
    "3. Prepare MVTec Carpet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa007598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, random, shutil, pathlib\n",
    "\n",
    "MVT_DIR = \"/kaggle/input/mvtec-ad/carpet/train/good\"\n",
    "INST_DIR = \"/kaggle/working/carpet_instances\"\n",
    "VAL_DIR  = \"/kaggle/working/carpet_val\"\n",
    "\n",
    "os.makedirs(INST_DIR, exist_ok=True)\n",
    "os.makedirs(VAL_DIR, exist_ok=True)\n",
    "\n",
    "imgs = sorted(glob.glob(f\"{MVT_DIR}/*\"))\n",
    "assert len(imgs) > 0, \"No MVTec 'carpet' images found.\"\n",
    "imgs = imgs[:150]\n",
    "\n",
    "for i, p in enumerate(imgs):\n",
    "    ext = os.path.splitext(p)[1]\n",
    "    shutil.copy2(p, f\"{INST_DIR}/{i:05d}{ext}\")\n",
    "\n",
    "sample = random.sample(imgs, min(4, len(imgs)))\n",
    "for i, p in enumerate(sample):\n",
    "    ext = os.path.splitext(p)[1]\n",
    "    shutil.copy2(p, f\"{VAL_DIR}/{i:02d}{ext}\")\n",
    "\n",
    "print('Prepared instance images:', len(imgs))\n",
    "print('Validation images:', len(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6728a6",
   "metadata": {},
   "source": [
    "4. Train LoRA on Carpet (DreamBooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c8a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, sys, subprocess\n",
    "\n",
    "BASE = \"runwayml/stable-diffusion-v1-5\"\n",
    "TOKEN = \"mvcarpet\"\n",
    "PROMPT = f\"photo of {TOKEN} object\"\n",
    "OUT = \"/kaggle/working/lora_sd15_carpet\"\n",
    "\n",
    "VAL = sorted([str(p) for p in pathlib.Path(\"/kaggle/working/carpet_val\").glob(\"*\")])\n",
    "\n",
    "cmd = [\n",
    "    sys.executable, \"/kaggle/working/train_dreambooth_lora.py\",\n",
    "    \"--pretrained_model_name_or_path\", BASE,\n",
    "    \"--instance_data_dir\", \"/kaggle/working/carpet_instances\",\n",
    "    \"--instance_prompt\", PROMPT,\n",
    "    \"--output_dir\", OUT,\n",
    "    \"--resolution\", \"512\",\n",
    "    \"--train_batch_size\", \"1\",\n",
    "    \"--gradient_accumulation_steps\", \"4\",\n",
    "    \"--learning_rate\", \"1e-4\",\n",
    "    \"--rank\", \"8\",\n",
    "    \"--mixed_precision\", \"fp16\",\n",
    "    \"--max_train_steps\", \"600\",\n",
    "    \"--checkpointing_steps\", \"200\",\n",
    "    \"--use_8bit_adam\",\n",
    "    \"--pre_compute_text_embeddings\",\n",
    "    \"--text_encoder_use_attention_mask\",\n",
    "    \"--enable_xformers_memory_efficient_attention\",\n",
    "]\n",
    "\n",
    "print(\"Running:\\n\", \" \".join(cmd))\n",
    "out = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "print(out.stdout)\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15ad4d",
   "metadata": {},
   "source": [
    "5. Load SD1.5 + LoRA and Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70aac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16 if device=='cuda' else torch.float32\n",
    ")\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, use_karras=True)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "pipe.load_lora_weights(\"/kaggle/input/sd1-5loracarpet/tensorflow2/default/1\",\n",
    "                       weight_name=\"pytorch_lora_weights.safetensors\")\n",
    "\n",
    "prompt = \"high quality texture photo of mvcarpet object, studio lighting, white background\"\n",
    "\n",
    "images = []\n",
    "for _ in range(4):\n",
    "    img = pipe(prompt, guidance_scale=10, num_inference_steps=100).images[0]\n",
    "    images.append(img)\n",
    "\n",
    "grid = Image.new(\"RGB\", (1024, 1024))\n",
    "for i, im in enumerate(images):\n",
    "    grid.paste(im, (512*(i%2), 512*(i//2)))\n",
    "\n",
    "display(grid)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4127a0a",
   "metadata": {},
   "source": [
    "6. Img2Img with LoRA for Defect-Free Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b873ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionImg2ImgPipeline, DPMSolverMultistepScheduler\n",
    "from PIL import Image\n",
    "\n",
    "LORA_DIR = \"/kaggle/working/lora_sd15_carpet/checkpoint-200\"\n",
    "TEST_IMG = \"/kaggle/input/mvtec-ad/carpet/test/cut/000.png\"\n",
    "\n",
    "prompt = \"photo of mvcarpet object, clean carpet, no defects, realistic texture\"\n",
    "negative_prompt = \"holes, stains, defects, damaged, weird textures, distortion, blur\"\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16\n",
    "\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=dtype,\n",
    "    safety_checker=None\n",
    ").to(device)\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(\"/kaggle/input/sd1-5loracarpet/tensorflow2/default/1\",\n",
    "                       weight_name=\"pytorch_lora_weights.safetensors\")\n",
    "pipe.fuse_lora(lora_scale=1.0)\n",
    "\n",
    "input_image = Image.open(TEST_IMG).convert(\"RGB\")\n",
    "input_image = input_image.resize((512, 512))\n",
    "\n",
    "result = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=input_image,\n",
    "    strength=0.2,\n",
    "    guidance_scale=10,\n",
    "    num_inference_steps=600,\n",
    ")\n",
    "\n",
    "clean_img = result.images[0]\n",
    "display(clean_img)\n",
    "clean_img.save(\"/kaggle/working/reconstructed_clean_carpet.png\")\n",
    "\n",
    "print(\"Saved at /kaggle/working/reconstructed_clean_carpet.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1c08c",
   "metadata": {},
   "source": [
    "7. Install CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0aadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c883a",
   "metadata": {},
   "source": [
    "8. Evaluation Loop for LoRA + Img2Img Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import clip\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1032c452",
   "metadata": {},
   "source": [
    "9. Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef38f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_mse(img, recon):\n",
    "    return float(np.mean((img - recon) ** 2))\n",
    "\n",
    "def clip_similarity_error(img, recon):\n",
    "    img_pil = Image.fromarray((img*255).astype(np.uint8))\n",
    "    recon_pil = Image.fromarray((recon*255).astype(np.uint8))\n",
    "    with torch.no_grad():\n",
    "        a = clip_model.encode_image(clip_preprocess(img_pil).unsqueeze(0).to(device))\n",
    "        b = clip_model.encode_image(clip_preprocess(recon_pil).unsqueeze(0).to(device))\n",
    "        sim = torch.cosine_similarity(a, b).item()\n",
    "    return float(1.0 - sim)\n",
    "\n",
    "def hybrid_score(img, recon, alpha=0.7):\n",
    "    return alpha * pixel_mse(img, recon) + (1-alpha) * clip_similarity_error(img, recon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2a86b7",
   "metadata": {},
   "source": [
    "10. Reconstruction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(img_np,\n",
    "                prompt,\n",
    "                negative_prompt,\n",
    "                strength,\n",
    "                guidance_scale,\n",
    "                num_inference_steps):\n",
    "\n",
    "    pil = Image.fromarray((img_np*255).astype(np.uint8)).convert(\"RGB\")\n",
    "    pil = pil.resize((512, 512))\n",
    "\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        out = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            image=pil,\n",
    "            strength=strength,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "        )\n",
    "\n",
    "    rec = np.array(out.images[0]).astype(np.float32) / 255.0\n",
    "    return rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c711df3",
   "metadata": {},
   "source": [
    "11. Visualization Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade937e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(img, recon, heat, idx):\n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.subplot(1,3,1); plt.imshow((img*255).astype(np.uint8)); plt.title(\"Original\"); plt.axis(\"off\")\n",
    "    plt.subplot(1,3,2); plt.imshow((recon*255).astype(np.uint8)); plt.title(\"Reconstruction\"); plt.axis(\"off\")\n",
    "    plt.subplot(1,3,3); plt.imshow((img*255).astype(np.uint8)); plt.imshow(heat, cmap='jet', alpha=0.4)\n",
    "    plt.title(\"Heatmap\"); plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1858c25",
   "metadata": {},
   "source": [
    "12. Full Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    category=\"carpet\",\n",
    "    resize=512,\n",
    "    limit_test=50,\n",
    "    prompt=\"photo of mvcarpet object, clean carpet, no defects, realistic texture\",\n",
    "    negative_prompt=\"holes, stains, defects, damaged\",\n",
    "    strength=0.25,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=100,\n",
    "    alpha=0.7,\n",
    "):\n",
    "\n",
    "    test_dir = f\"/kaggle/input/mvtec-ad/{category}/test\"\n",
    "    classes = sorted(os.listdir(test_dir))\n",
    "\n",
    "    test_imgs, test_labels, file_paths = [], [], []\n",
    "\n",
    "    for defect_type in classes:\n",
    "        img_dir = os.path.join(test_dir, defect_type)\n",
    "        files = sorted(os.listdir(img_dir))[:limit_test]\n",
    "\n",
    "        for fname in files:\n",
    "            path = os.path.join(img_dir, fname)\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (resize, resize)).astype(np.float32)/255.0\n",
    "            test_imgs.append(img)\n",
    "            test_labels.append(0 if defect_type == \"good\" else 1)\n",
    "            file_paths.append(path)\n",
    "\n",
    "    test_imgs = np.array(test_imgs)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    print(\"Loaded test images:\", len(test_imgs))\n",
    "\n",
    "    scores, ssim_vals = [], []\n",
    "\n",
    "    for i in tqdm(range(len(test_imgs))):\n",
    "        img = test_imgs[i]\n",
    "\n",
    "        recon = reconstruct(\n",
    "            img, prompt, negative_prompt,\n",
    "            strength, guidance_scale, num_inference_steps\n",
    "        )\n",
    "\n",
    "        recon = cv2.resize((recon*255).astype(np.uint8), (resize,resize)).astype(np.float32)/255.0\n",
    "\n",
    "        s = hybrid_score(img, recon, alpha)\n",
    "        scores.append(s)\n",
    "\n",
    "        try:\n",
    "            ssim_val = 1.0 - ssim(img, recon, channel_axis=2)\n",
    "        except:\n",
    "            ssim_val = np.nan\n",
    "        ssim_vals.append(ssim_val)\n",
    "\n",
    "        if i < 5:\n",
    "            heat = np.mean(np.abs(img - recon), axis=2)\n",
    "            heat = heat / (heat.max()+1e-6)\n",
    "            show_result(img, recon, heat, i)\n",
    "\n",
    "    scores, ssim_vals = np.array(scores), np.array(ssim_vals)\n",
    "\n",
    "    auc_val = roc_auc_score(test_labels, scores)\n",
    "    prec, rec, _ = precision_recall_curve(test_labels, scores)\n",
    "    auprc = auc(rec, prec)\n",
    "\n",
    "    print(f\"\\n==== Final Results ({category}) ====\")\n",
    "    print(\"AUROC:\", auc_val)\n",
    "    print(\"AUPRC:\", auprc)\n",
    "    print(\"Mean (1-SSIM):\", np.nanmean(ssim_vals))\n",
    "\n",
    "    return {\n",
    "        \"auc\": auc_val,\n",
    "        \"auprc\": auprc,\n",
    "        \"scores\": scores,\n",
    "        \"labels\": test_labels,\n",
    "        \"ssim\": ssim_vals,\n",
    "        \"files\": file_paths\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e0a5f",
   "metadata": {},
   "source": [
    "13. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0089c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = \"carpet\"\n",
    "RESIZE = 512\n",
    "LIMIT_TEST = 50\n",
    "\n",
    "PROMPT = \"photo of mvcarpet object, clean carpet, no defects, realistic texture\"\n",
    "NEGATIVE_PROMPT = \"holes, stains, defects, damaged, distortion\"\n",
    "\n",
    "STRENGTH = 0.25\n",
    "GUIDANCE = 7.5\n",
    "STEPS = 100\n",
    "ALPHA = 0.7\n",
    "\n",
    "results = evaluate(\n",
    "    category=CATEGORY,\n",
    "    resize=RESIZE,\n",
    "    limit_test=LIMIT_TEST,\n",
    "    prompt=PROMPT,\n",
    "    negative_prompt=NEGATIVE_PROMPT,\n",
    "    strength=STRENGTH,\n",
    "    guidance_scale=GUIDANCE,\n",
    "    num_inference_steps=STEPS,\n",
    "    alpha=ALPHA\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
